---
title: "Modelling Flow"
author: "Matthew Ross"
date: "2024-04-24"
output: html_document
---

```{r setup, include=FALSE}

library(tidyverse)
library(rpart)
library(rpart.plot)
library(yardstick)
knitr::opts_chunk$set(echo = TRUE)
```

# Modelling Flow

Now that we have explored individual correlations between long-term flow 
characteristics and potential drivers of those characteristics (climate,
soils, etc...), we can start to build out increasingly complex models
to predict flow characteristics. 

# Assignment


## Build a parsimonious linear model

Pick one of the flow characteristics that mosts interests you and use the `lm`
function to build a parsimonious model that predicts your favorite characteristic. What is parsimony? It's a model that is complex enough to
capture the major controls, but simple enough to be understandable. You
will need to explore, read, understand the differences between a + sign interaction, a ":" interaction and a * interaction in `lm` terminology. 

Please report your R2, slope estimates, and p-value of your model and 
write out your interpretation of these numbers. 

```{r}

q_mean <- read_delim('data/hydro.txt', delim = ';') %>%
  select(gauge_id, q_mean) %>%
  inner_join(read_delim('data/climate.txt', delim = ';')) %>%
  inner_join(read_delim('data/soil.txt',delim = ';'))


q_mean_logs <- q_mean %>%
  mutate(p_log10 = log10(p_mean),
         aridlog10 = log10(aridity),
         q_mean10 = log10(q_mean))

naive_mod <- lm(q_mean10 ~ aridlog10 * p_log10, data = q_mean_logs)




```


## Build a CART model to predict flow. 

Linear models help us both predict and understand drivers of change, machine learning can help us understand drivers of change, but as a technique it is 
more suited to accurate predictions. CART or Classification and Regression Trees
are a nice intermediate between lms and ml. Tons of resources for this but
[CART Logic](https://koalaverse.github.io/machine-learning-in-R/decision-trees.html#cart-software-in-r), provides a good conceptual overview, and [CART demo](https://www.statmethods.net/advstats/cart.html) provides a good enough code demo. 

Read the logic intro above, and the code demo as well, to build a CART model 
version of your lm. Use the code to visualize your CART output. 

```{r}


library(rpart)
library(rpart.plot)
library(yardstick)
q_4 <- q_mean_logs %>%
  select(q_mean10, p_log10, aridlog10, sand_frac,
         clay_frac, soil_depth_pelletier) %>%
  mutate(q_class = cut_number(q_mean10,n = 4)) %>%
  select(-q_mean10)

train <- q_4 %>%
  sample_frac(.7)

test <- q_4 %>%
  anti_join(train)

cart_simple <- rpart(q_class ~., data = train, cp = 0.001,
                     method = 'class')


plot(cart_simple)
text(cart_simple, cex = 0.8, use.n = TRUE, xpd = TRUE)


test$pred <- predict(cart_simple, test, 'class')
cm <- conf_mat(test, q_class,pred)

autoplot(cm, type = "heatmap") +
  scale_fill_gradient(low="#D6EAF8",high = "#2E86C1")

accuracy(test,q_class,pred)
```


## Build a RandomForest

CARTs are a single tree, what if we had thousands? Would we get better performance (yes!)

The same CART logic site above introduces random forests as well. Please 
read this part of the site and use the code demo to build your own RandomForest.
Remember, for a RandomForest type model we want to make sure we split our data
at least into train and test datasets and ideally into train-test-val. 




```{r}

```

