---
title: "Modelling Flow"
author: "Matthew Ross"
date: "2024-04-24"
output: html_document
---

```{r setup, include=FALSE}

library(tidyverse)
knitr::opts_chunk$set(echo = TRUE)

```

# Modelling Flow

Now that we have explored individual correlations between long-term flow 
characteristics and potential drivers of those characteristics (climate,
soils, etc...), we can start to build out increasingly complex models
to predict flow characteristics. 

# Assignment


## Build a parsimonious linear model

Pick one of the flow characteristics that mosts interests you and use the `lm`
function to build a parsimonious model that predicts your favorite characteristic. What is parsimony? It's a model that is complex enough to
capture the major controls, but simple enough to be understandable. You
will need to explore, read, understand the differences between a + sign interaction, a ":" interaction and a * interaction in `lm` terminology. 

Please report your R2, slope estimates, and p-value of your model and 
write out your interpretation of these numbers. 

```{r}
model_variables <- inner_join(cq_mf, th_mf)
```

```{r}
# png(filename = "bigMeanModelplot.png", width = 10, height = 8, units = "in", res = 300)
# model_variables %>% 
#   select_if(is.numeric) %>% 
#   ggpairs()
# dev.off()
```

```{r}
ggplot(model_variables, aes(x = p_mean, y = q_mean))+
  geom_point()+
  geom_smooth(method = "lm", se = F)

ggplot(model_variables, aes(x = high_prec_freq, y = q_mean))+
  geom_point()+
  geom_smooth(method = "lm", se = F)

#lm(y ~ x1 + x2 + x1:x2)

test <- lm(q_mean ~ p_mean * high_prec_freq, data = model_variables)

summary(test)

```

R2 - 0.8728
Slope Estimates:
p_mean                             -2.292096
high_prec_freq                      0.069741
p_mean:high_prec_freq              -0.044238

p-value - < 2.2e-16

These results tell us that with a one unit increase of p_mean, q_mean will increase by -2.292096
and that with with a one unit increase of high_prec_freq, q_mean will increase by .069741

The p-value is very low, indicating that the relationship observed between the predictor and dependent variables is significant


###Same model but with the logged p and slope values not entirely sure  
```{r}
logged_slope <- model_variables %>% 
  mutate(log_slope_mean = log(slope_mean)) %>% 
  mutate(log_p_mean = log(p_mean))

ggplot(logged_slope, aes(x = log_slope_mean, y = q_mean))+
  geom_point()+
  geom_smooth(method = "lm", se = F)

ggplot(logged_slope, aes(x = log_p_mean, y = q_mean))+
  geom_point()+
  geom_smooth(method = "lm", se = F)

#lm(y ~ x1 + x2 + x1:x2)

test <- lm(q_mean ~ p_mean + log_slope_mean, data = logged_slope)

summary(test)
```


R2 - 0.8611

Slope estimates:
  p_mean - 0.90938
  log_slope_mean - 0.32776

p-value - < 2.2e-16

Applying a log to the slope_mean field yielded similar results to the original model.


```{r}
###In Class Modelling

q_mean <- inner_join(cq_mf, sh_mf)

q_mean_logs <- q_mean %>% 
  mutate(p_log10 = log10(p_mean),
         aridlog10 = log10(aridity),
         q_mean10 = log10(q_mean),
         k_10 = log10(soil_conductivity))


naive_mod <- lm(q_mean10 ~ aridlog10 * p_log10, data = q_mean_logs)
  
summary(naive_mod)

ggplot(q_mean, aes(x = p_mean, y = q_mean, color = aridity))+
  scale_x_log10()+
  scale_y_log10()+
  geom_point()+
  geom_smooth(method = "lm")


hist(log10(q_mean$soil_conductivity))
```


```{r}
soil_rr <- inner_join(soil,hydro %>% 
                   select(gauge_id, runoff_ratio))

r_ratio <- inner_join(cq_rr,soil_rr)



naive_mod2 <- lm(runoff_ratio ~ p_seasonality * soil_porosity , data = r_ratio)

summary(naive_mod2)


#this is worse and seems wrong, couldnt log10 p_seasonailty because it has negative values

rr_logs <- r_ratio %>%
  mutate(porouslog10 = log10(soil_porosity),
         rr10 = log10(runoff_ratio))

naive_mod3 <- lm(rr10 ~ p_seasonality * porouslog10 , data = rr_logs)

summary(naive_mod3)

```


```{r}
ggplot(r_ratio, aes(x = p_seasonality, y = runoff_ratio, color = soil_porosity))+
  geom_point()+
  geom_smooth(method = "lm")


```



=======

```

>>>>>>> cc9960193fe9082e5d94cc2db4f3f6f2a143f480

## Build a CART model to predict flow. 

Linear models help us both predict and understand drivers of change, machine learning can help us understand drivers of change, but as a technique it is 
more suited to accurate predictions. CART or Classification and Regression Trees
are a nice intermediate between lms and ml. Tons of resources for this but
[CART Logic](https://koalaverse.github.io/machine-learning-in-R/decision-trees.html#cart-software-in-r), provides a good conceptual overview, and [CART demo](https://www.statmethods.net/advstats/cart.html) provides a good enough code demo. 

Read the logic intro above, and the code demo as well, to build a CART model 
version of your lm. Use the code to visualize your CART output. 

```{r}
#library(rpart)
cart_mod <- rpart(q_mean ~ p_mean + runoff_ratio + high_prec_freq, data = cq, method = "anova")

plotcp(cart_mod)

summary(cart_mod)

rsq.rpart(cart_mod)

plot(cart_mod, uniform = TRUE,
     main = "Regression Tree for Mean Flow")

text(cart_mod, use.n = TRUE, all = TRUE, cex = .5)

```


## Build a RandomForest

CARTs are a single tree, what if we had thousands? Would we get better performance (yes!)

The same CART logic site above introduces random forests as well. Please 
read this part of the site and use the code demo to build your own RandomForest.
Remember, for a RandomForest type model we want to make sure we split our data
at least into train and test datasets and ideally into train-test-val. 




```{r}


```

